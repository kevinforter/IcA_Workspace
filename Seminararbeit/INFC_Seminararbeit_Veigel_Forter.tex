
%
% FH Technikum Wien
% !TEX encoding = UTF-8 Unicode
%
% Version V2.24 von 2024-12-19 otrebski
%
\documentclass[BMR,Seminar,ngerman,IEEE]{twbook}
\renewcommand*{\degreecourse}{FH Technikum Wien, Bachelorstudiengang BIF\\Lehrveranstaltung BIF-VZ-5-WS2025-INFC-EN}

% --- Encoding & fonts ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

% --- Graphics, figures, urls, lists, tables, code ---
\usepackage{float}
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{enumitem}
\usepackage{listings}
\usepackage[final]{microtype} % hilft gegen Overfull/Underfull
\emergencystretch=2em         % sanfter Puffer für Zeilenumbruch

% Screenshots hier hinein legen:
\graphicspath{{screenshots/}}

% --- Bibliography (twbook lädt biblatex via Hook, sobald inputenc geladen ist) ---
\addbibresource{Literatur.bib}

% --- Listings (inkl. YAML) ---
\lstdefinelanguage{yaml}{
  keywords={true,false,null,y,n},
  comment=[l]{\#},
  morestring=[b]',
  morestring=[b]",
  sensitive=false,
  showstringspaces=false,
  literate =    {---}{{\textcolor{gray}{---}}}3
                {>}{{\textcolor{gray}{>}}}1
                {|}{{\textcolor{gray}{|}}}1
                {:}{{\textcolor{blue}{:}}}1
                {-}{{\textcolor{gray}{-}}}1,
}
\lstset{
  basicstyle=\ttfamily\small,
  frame=single,
  breaklines=true,
  backgroundcolor=\color{gray!5},
  captionpos=b
}

% --- Convenience figure macros ---
\newcommand{\screenshotH}[3]{%
  \begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{#1}%
    \caption{#2}%
    \label{fig:#3}%
  \end{figure}%
}
\newcommand{\placeholderfig}[2]{%
  \begin{figure}[htbp]
    \centering
    \fbox{\rule{0pt}{120pt}\rule{0.9\linewidth}{0pt}}%
    \caption{#1}%
    \label{fig:#2}%
  \end{figure}%
}

% Die nachfolgenden Pakete stellen sonst nicht benötigte Features zur Verfügung
\usepackage{blindtext}

% --- Hyperref am Schluss laden (robuster) ---
\usepackage{hyperref}

%
% Einträge für Deckblatt, Kurzfassung, etc.
%
\title{Class Workshop (Docker \& Portainer)}
\author{David Veigel \and Kevin Forter}
\studentnumber{XXXXXXXXXXXXXXX} % ggf. beide Nummern: 12345678, 87654321
\place{Wien}
% \acknowledgements{\blindtext}

\begin{document}

\maketitle

% ============================================================================
\onecolumn
\newpage

% ============================================================================
\chapter{Überblick}
Ziel: Provisionierung von AWS-Ressourcen, Aufbau eines Docker~Swarm-Clusters, Installation von Portainer und Analyse von Skalierung und Ausfallszenarien.

% ============================================================================
\chapter{Provisionierung der AWS-Ressourcen}
\subsection{Erstellung VPC}
Neue VPC mit öffentlichem Subnet über den Assistenten im AWS-VPC-Dashboard erstellen. Security Group mit folgenden offenen Ports konfigurieren:
\screenshotH{vpc_configuration.png}{Einstellung der VPC inklusive manueller CIDR-Einstellung 10.0.0.0/16}{vpc-overview}
\FloatBarrier

\subsection{Erstellung Subnet}
\screenshotH{subnetsettings.png}{Öffentliches Subnet mit Subnet-CIDR-Block 10.0.1.0/24}{subnet-overview}
\FloatBarrier
\screenshotH{auto_public_ipv4.png}{Automatische Vergabe der öffentlichen IPv4-Adresse aktiviert}{auto-public-ipv4}
\FloatBarrier

\subsection{Erstellung Sicherheitsgruppe}
\begin{itemize}[leftmargin=*]
  \item 22/TCP – SSH (öffentlich)
  \item 80/TCP – HTTP (öffentlich)
  \item 2377/TCP, 7946/TCP/UDP, 4789/UDP – innerhalb der SG
\end{itemize}

Die Sicherheitsgruppe musste in zwei Schritten erstellt werden, damit sie den Vorgaben entspricht.

Schritt 1: Zuerst musste die Sicherheitsgruppe nur mit SSH und HTTP erstellt werden
\screenshotH{create_inbound_rules.png}{Erstellung der Sicherheitsgruppe mit SSH und HTTP}{create-inbound-rules}
\FloatBarrier
Schritt 2: Erst nach dem Erstellen der Sicherheitsgruppe konnten wir für die Docker-Regeln die eigene Sicherheitsgruppe als Quelle angeben.
\screenshotH{edit_inbound_rules.png}{Anpassung der zuvor erstellten Sicherheitsgruppe mit zusätzlichen Regeln für Docker}{edit-inbound-rules}
\FloatBarrier

% ============================================================================
\subsection{EC2-Instanzen (Nodes)}

\screenshotH{create_instance.png}{Erstellung der ersten Instanz in AWS}{create-instance}
\FloatBarrier
\screenshotH{operating_system.png}{AMI: Ubuntu (neueste Version).}{operating-system}
\FloatBarrier
\screenshotH{instance_type.png}{\texttt{t2.micro} (oder vergleichbar) als Instance Type}{instance-type}
\FloatBarrier
\screenshotH{networksettings.png}{Aktivierung von \textbf{Auto-assign Public IP}.}{networksettings}
\FloatBarrier
\screenshotH{userdata.png}{Einfügen des User-Data-Skripts}{userdata}
\FloatBarrier

\begin{lstlisting}[language=bash,caption={EC2 User Data – Docker Installation}]
#!/bin/bash
curl -o /home/ubuntu/install-docker.sh https://get.docker.com/
sh /home/ubuntu/install-docker.sh
\end{lstlisting}

\screenshotH{instance_summary.png}{EC2 Launch – Übersicht der Einstellungen}{ec2-launch}
\FloatBarrier

\subsection{Validierung}
\begin{lstlisting}[language=bash]
ssh -i <key.pem> ubuntu@<PUBLIC_IP>
docker --version
\end{lstlisting}
\screenshotH{ssh_connection.png}{SSH-Verbindung und Docker-Version}{ssh-docker}
\FloatBarrier

% ============================================================================
\subsection{Docker Swarm Setup}
\begin{lstlisting}[language=bash]
docker swarm init --advertise-addr <PRIVATE_IP>
docker node ls
\end{lstlisting}

\screenshotH{docker_swarm_init.png}{\texttt{docker swarm init} auf der ersten Instanz}{docker-swarm-init}
\FloatBarrier
\screenshotH{docker_node_ls.png}{\texttt{docker node ls} auf einer Instanz}{node-ls-1-instance}
\FloatBarrier
\screenshotH{join_worker.png}{Hinzufügen von zwei Workern}{add-two-worker}
\FloatBarrier
\screenshotH{manager_token.png}{Generieren des Manager-Tokens}{manager-token}
\FloatBarrier
\screenshotH{join_manager.png}{Hinzufügen eines zweiten Managers}{second-manager}
\FloatBarrier

\screenshotH{node-ls_all.png}{\texttt{docker node ls} mit 4 Knoten}{node-ls}
\FloatBarrier

% ============================================================================
\subsection{Portainer Installation}
\begin{lstlisting}[language=bash]
curl -L https://downloads.portainer.io/ce-lts/portainer-agent-stack.yml -o portainer-agent-stack.yml
docker stack deploy -c portainer-agent-stack.yml portainer
\end{lstlisting}

\screenshotH{portainer_installation.png}{Portainer-Installation über die Kommandozeile}{portainer-installation}
\FloatBarrier

\screenshotH{portainer_login.png}{Erstellung eines neuen Logins für Portainer}{new-login}
\FloatBarrier
\screenshotH{services.png}{Angezeigte Services nach der Installation von Portainer}{standard-services}
\FloatBarrier

% ============================================================================
\subsection{Service-Deployment, Scale-Out und Scale-In}
\begin{lstlisting}[language=bash]
docker service create \
  --name hello \
  --replicas 3 \
  --publish published=80,target=80 \
  karthequian/helloworld:latest
\end{lstlisting}
\screenshotH{portainer_create_service.png}{Service mit 3 Replikas und Port 80 erstellen}{create-service}
\FloatBarrier
\screenshotH{3_replicas.png}{Die 3 Replikas erhalten jeweils einen eigenen Slot}{three-replicas}
\FloatBarrier
\screenshotH{portainer_cluster_3rep.png}{Die 3 Replikas werden auf die 4 verschiedenen Nodes in AWS verteilt}{distribution-3-replicas}
\FloatBarrier

\screenshotH{hello_world.png}{Hello-World-Seite nach Deployment des Service}{hello-world}
\FloatBarrier

\begin{lstlisting}[language=bash]
docker service scale hello=10
\end{lstlisting}

\screenshotH{10_replicas.png}{Die 10 Replikas werden nach dem Hochskalieren auf verschiedene Slots verteilt}{ten-replicas}
\FloatBarrier
\screenshotH{portainer_cluster_10rep.png}{Die 10 Replikas werden auf die 4 verschiedenen Nodes verteilt}{distribution-10-replicas}
\FloatBarrier

\screenshotH{hello_world_two_containers.png}{Hello-World-Seite wird nun von verschiedenen Containern bedient}{hello-world-2-containers}
\FloatBarrier

% ACHTUNG: Datei ohne Leerzeichen benennen (z. B. shutdown-node.png)
\screenshotH{shutdown node.png}{Node wird als \enquote{down} angezeigt, nachdem eine EC2 heruntergefahren wurde}{shutdown-instance}
\FloatBarrier

\screenshotH{services_ready_status.png}{Services auf dem betroffenen Node wechselten erst nach \enquote{ready}, dann nach \enquote{shutdown}, neue Replikas wurden auf andere Nodes verteilt}{status-ready}
\FloatBarrier
\screenshotH{portainer_down_but10.png}{Node down mit zwei Services; auf den restlichen drei Nodes laufen insgesamt 10 Services}{node-down-10-services}
\FloatBarrier

\screenshotH{shutdown_services.png}{Nach dem Hochfahren wurden die übrigen Services wieder sauber auf alle 4 Nodes verteilt}{still-shutdown}
\FloatBarrier

\screenshotH{portainer_scale_down.png}{Beim Scale-Down wurden die verbleibenden 3 Services erneut gleichmäßig verteilt}{scale-down}
\FloatBarrier

% ============================================================================
\subsection{Kurzantworten}
\begin{itemize}[leftmargin=*]
  \item \textbf{Skalierung des Clusters:}\\
  Skalieren Sie Ihren Cluster von 3 auf 10 Maschinen.\\
  \textit{Beobachtung:} Beim Öffnen der Hello-World-Seite wird jeweils ein anderer Container angezeigt.
  \item \textbf{Wiederinbetriebnahme eines Workers:}\\
  Wenn Sie den Worker wieder online schalten, werden die Services automatisch auf alle vier Nodes verteilt.
  \item \textbf{Erneute Skalierung:}\\
  Bei einer erneuten Skalierung werden die drei Services erneut gleichmäßig auf die vier Nodes verteilt.
\end{itemize}

% ============================================================================
\subsection{Anhang – Wichtige Befehle}
\begin{lstlisting}[language=bash]
docker service ls
docker service ps hello
docker node ls
docker service scale hello=10
\end{lstlisting}

% ============================================================================
\chapter{Kubernetes Deployment Workshop mit Minikube}

Ziel: In diesem Workshop wurde ein Kubernetes-Cluster mit \texttt{Minikube} erstellt, ein \texttt{nginx}-Deployment angelegt und verschiedene Service-Typen (ClusterIP, NodePort, LoadBalancer) getestet.

% ----------------------------------------------------------------------------
\subsection{Installtion Minikube und Kubectl}
\begin{lstlisting}[language=bash,caption={Installation von Minikube}]
curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64
\end{lstlisting}
\screenshotH{minikube_install.png}{Download und Installtion von Minikube}{minikube-install}
\FloatBarrier

\begin{lstlisting}[language=bash,caption={Installation von Kubectl}]
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
\end{lstlisting}
\screenshotH{kubectl_install.png}{Download und Installation von Kubectl}{kubectl-install}
\FloatBarrier

% ----------------------------------------------------------------------------
\subsection{Start und Vorbereitung}
\begin{lstlisting}[language=bash,caption={Start von Minikube und Überprüfung der Umgebung}]
minikube start
kubectl get nodes
\end{lstlisting}
\screenshotH{minikube_start_2mb.png}{Start von Minikube mit einer einzelnen Node (2GB Ram da sonst alles gebraucht wird)}{minikube-start}
\FloatBarrier
\screenshotH{minikube_get_nodes.png}{Alle laufenden Notes}{minikube-get-nodes}
\FloatBarrier

% ----------------------------------------------------------------------------
\subsection{Deployment erstellen und skalieren}
\begin{lstlisting}[language=bash,caption={nginx-Deployment mit einem Pod erstellen}]
kubectl create deployment nginx --image=nginx:stable
kubectl get deployments
kubectl get pods
\end{lstlisting}
\screenshotH{minikube_create_deployment.png}{Erstellung des nginx-Deployments}{nginx-deploy}
\FloatBarrier
\screenshotH{minikube_get_deployments.png}{Anzeigen des erstellten Deployments}{get-deployments}
\FloatBarrier

\begin{lstlisting}[language=bash,caption={Deployment auf vier Replikas skalieren}]
kubectl scale deployment nginx --replicas=4
kubectl get pods
\end{lstlisting}
\screenshotH{minikube_scale_deployment.png}{Vier Replikas des nginx-Deployments sind aktiv}{scale-4replicas}
\FloatBarrier
\screenshotH{minikube_get_pods.png}{Anzeigen der erstellten Pods}{nginx-get-pods}
\FloatBarrier

% ----------------------------------------------------------------------------
\subsection{Testzugriff auf einen Pod}
\begin{lstlisting}[language=bash,caption={Port-Forwarding für lokalen Zugriff}]
kubectl expose deployment nginx --type=NodePort --Port=8080
\end{lstlisting}
\screenshotH{minikube_expose_deployment.png}{Erstelltes Dopleyment auf Port 8080 exposen}{expose-deployment}
\FloatBarrier
\screenshotH{minikube_reach_service.png}{Browser öffnen mit Zugriff auf Pod}{Reach-service}
\FloatBarrier

% ----------------------------------------------------------------------------
\subsection{ClusterIP Service (intern)}
\begin{lstlisting}[language=bash,caption={Erstellung eines internen ClusterIP-Service}]
kubectl expose deployment nginx \
  --name=nginx-clusterip \
  --type=ClusterIP \
  --port=8080 \
  --target-port=80
kubectl get svc nginx-clusterip
\end{lstlisting}
\screenshotH{minikube_expose_clusterip.png}{Interner ClusterIP-Service für das nginx-Deployment}{clusterip}
\FloatBarrier

\begin{lstlisting}[language=bash,caption={Test im Cluster über temporären Curl-Pod}]
kubectl run test-client --rm -it --image=curlimages/curl --restart=Never -- sh
curl -I http://nginx-clusterip:8080
\end{lstlisting}
\screenshotH{minikube_test_reach_cluster.png}{Erfolgreicher Curl-Test des internen ClusterIP-Service}{clusterip-test}
\FloatBarrier

% ----------------------------------------------------------------------------
\subsection{NodePort Service (extern)}
\begin{lstlisting}[language=bash,caption={Erstellung eines NodePort-Service}]
kubectl expose deployment nginx \
  --name=nginx-nodeport \
  --type=NodePort \
  --port=8080 \
  --target-port=80
kubectl get svc nginx-nodeport
\end{lstlisting}
\screenshotH{minikube_expose_nodeport.png}{NodePort-Service mit zugewiesenem externen Port (z.\,B. 31245)}{nodeport-created}
\FloatBarrier

\begin{lstlisting}[language=bash,caption={Abrufen der externen URL über Minikube}]
minikube service nginx-nodeport --url
\end{lstlisting}
\screenshotH{minikube_service_nodeport_url.png}{Von Minikube generierte URL für NodePort-Zugriff}{minikube-url}
\FloatBarrier

Zugriff im Browser oder mit \texttt{curl}:
\begin{lstlisting}[language=bash]
curl -I $(minikube service nginx-nodeport --url)
\end{lstlisting}
\screenshotH{minikube_curl_nodeport_url.png}{Erfolgreicher Curl-Test des internen Nodeport-Service}{Nodeport-test}
\FloatBarrier

% ----------------------------------------------------------------------------
\subsection{LoadBalancer Service (simuliert)}
\begin{lstlisting}[language=bash,caption={Erstellung eines LoadBalancer-Service}]
kubectl expose deployment nginx \
  --name=nginx-lb \
  --type=LoadBalancer \
  --port=8080 \
  --target-port=80
\end{lstlisting}
\screenshotH{minikube_expose_loadbalancer.png}{LoadBalancer-Service mit zugewiesenem externen Port (z.\,B. 31245)}{loadbalancer-created}
\FloatBarrier

Da Minikube keinen echten Cloud-Load-Balancer bereitstellt, wird die Funktion über einen lokalen Tunnel simuliert.
\begin{lstlisting}[language=bash,caption={Start des Minikube-Tunnels (neues Terminal)}]
minikube tunnel
\end{lstlisting}
\screenshotH{minikube_tunnel.png}{Simulierter LoadBalancer über Minikube-Tunnel}{minikube-tunnel}
\FloatBarrier

Prüfen der externen IP:
\begin{lstlisting}[language=bash]
kubectl get svc nginx-lb
\end{lstlisting}
\screenshotH{minikube_get_svc_loadbalancer.png}{LoadBalancer mit zugewiesener externer IP-Adresse}{loadbalancer-ip}
\FloatBarrier

\begin{lstlisting}[language=bash,caption={Zugriff über die externe IP}]
curl -I http://<EXTERNAL-IP>:8080
\end{lstlisting}
\screenshotH{minikube_curl_loadbalancer.png}{Erfolgreicher Zugriff über den LoadBalancer-Service}{loadbalancer-access}
\FloatBarrier

% ----------------------------------------------------------------------------
\subsection{Vergleich der Service-Typen}
\begin{itemize}
  \item \textbf{ClusterIP (Default):} Erzeugt eine virtuelle, \emph{interne} Cluster-IP. Service ist nur \emph{innerhalb} des Clusters erreichbar. Geeignet für Service-zu-Service-Kommunikation (z.\,B. Backend).
  \item \textbf{NodePort:} Öffnet zusätzlich auf jedem Node einen Port (Standardbereich \texttt{30000{-}32767}). Der Service ist \emph{außerhalb} des Clusters via \texttt{<NodeIP>:<NodePort>} erreichbar. Einfach, aber eingeschränkte Flexibilität und keine integrierte L7-Funktionen.
  \item \textbf{LoadBalancer:} Fragt (in Cloud-Umgebungen) einen externen L4-Load-Balancer an und erhält eine \emph{EXTERNAL-IP}. In Minikube wird dies über \texttt{minikube service} oder \texttt{minikube tunnel} simuliert. Ideal für produktiven Ingress-Traffic pro Service (L4).
\end{itemize}
\begin{table*}[t]
\centering
\begin{tabularx}{\textwidth}{@{}p{2.8cm}p{5cm}X@{}}
\toprule
\textbf{Typ} & \textbf{Erreichbarkeit} & \textbf{Einsatzgebiet / Beschreibung} \\
\midrule
ClusterIP & Nur innerhalb des Clusters & Standardtyp; ermöglicht Kommunikation zwischen Pods oder internen Diensten. \\
NodePort & Von außen über Node-IP und festen Port erreichbar & Zugriff über \texttt{\textless NodeIP\textgreater:\textless NodePort\textgreater} (z.\,B. für Tests). \\
LoadBalancer & Öffentliche IP / Cloud-Load-Balancer & Für produktive Setups; verteilt Traffic über alle Replikas. \\
\bottomrule
\end{tabularx}
\caption{Kubernetes Service-Typen}
\end{table*}
\FloatBarrier

% ----------------------------------------------------------------------------
\subsection{YAML-Spezifikationen}
\begin{lstlisting}[language=yaml,caption={Deployment YAML – nginx}]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:stable
          ports:
            - containerPort: 80
\end{lstlisting}

\begin{lstlisting}[language=yaml,caption={Service YAML – LoadBalancer}]
apiVersion: v1
kind: Service
metadata:
  name: nginx-lb
  labels:
    app: nginx
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - name: http
      port: 8080
      targetPort: 80
\end{lstlisting}

\begin{lstlisting}[language=yaml,caption={Kombinierte YAML-Datei (Deployment + alle Service)}]
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:stable
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-clusterip
  labels:
    app: nginx
spec:
  type: ClusterIP
  selector:
    app: nginx
  ports:
    - name: http
      port: 8080
      targetPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-nodeport
  labels:
    app: nginx
spec:
  type: NodePort
  selector:
    app: nginx
  ports:
    - name: http
      port: 8080
      targetPort: 80
      nodePort: 30080
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-lb
  labels:
    app: nginx
spec:
  type: LoadBalancer
  selector:
    app: nginx
  ports:
    - name: http
      port: 8080
      targetPort: 80
\end{lstlisting}

% ----------------------------------------------------------------------------
\subsection{Zusammenfassung}
\begin{itemize}[leftmargin=*]
  \item \textbf{Cluster:} Ein lokales Kubernetes-Cluster mit Minikube wurde gestartet.
  \item \textbf{Deployment:} Ein nginx-Deployment mit vier Replikas wurde erstellt.
  \item \textbf{Service-Typen:} ClusterIP, NodePort und LoadBalancer wurden erfolgreich getestet.
  \item \textbf{Zugriff:} Über Port-Forwarding, NodePort und Minikube-Tunnel konnte der Webserver erreicht werden.
  \item \textbf{Fazit:} Der Workshop verdeutlicht, wie Deployments und Services in Kubernetes strukturiert sind und welche Service-Typen für interne bzw. externe Zugriffe geeignet sind.
\end{itemize}

\end{document}